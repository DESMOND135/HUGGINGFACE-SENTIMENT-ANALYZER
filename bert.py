# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZxPccBwALJMlbML7lc5kKl4S1I6_-RpR

**FINE-TUNING PRETRAINED BERT FOR SENTIMENT CLASSIFICATION WITH HUGGING FACE**
"""

from google.colab import drive
drive.mount('/content/drive')

!ls

"""# New section"""

# Example: Texts to classify
texts = [
    "This product is great!",
    "I am very disappointed with the service.",
    "The quality of this item is okay, not bad."
]

# Import the Hugging Face transformers pipeline function
from transformers import pipeline

# Load a pretrained sentiment analysis pipeline (uses a BERT-based model by default)
sentiment_pipeline = pipeline("sentiment-analysis")

# Run sentiment analysis on the input list of texts
sentiment_pipeline(texts)

# Import the pipeline function from Hugging Face Transformers
from transformers import pipeline

# Load a specific pretrained sentiment analysis model (BERTweet) from Hugging Face
specific_model = pipeline(model="finiteautomata/bertweet-base-sentiment-analysis")

# Run the sentiment analysis pipeline on a list of text inputs
specific_model(texts)  # 'texts' should be a list of strings, e.g., ["I love this!", "This is bad."]

# Import the pandas library to handle data operations
import pandas as pd

# Load the IMDB dataset from a parquet file stored in Hugging Face Datasets
# The dataset is used for sentiment analysis
df = pd.read_parquet("hf://datasets/stanfordnlp/imdb/plain_text/train-00000-of-00001.parquet")

# Display the first few rows of the DataFrame to inspect the loaded data
df.head()



# Select only the 'text' and 'label' columns from the dataset for sentiment classification
df = df[['text', 'label']]

# Display the first few rows of the dataset to ensure it's loaded correctly
df.head()

# Train-Test Split
from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

#  Convert to Hugging Face Datasets
from datasets import Dataset

train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))
test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))

#  Tokenize
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=True)

tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_test = test_dataset.map(preprocess_function, batched=True)

#  Rename 'label' column to 'labels'
tokenized_train = tokenized_train.rename_column("label", "labels")
tokenized_test = tokenized_test.rename_column("label", "labels")

# Import the AutoTokenizer class from the Hugging Face transformers library
from transformers import AutoTokenizer

# Load the pre-trained DistilBERT tokenizer. This tokenizer is used to tokenize the input text
# The model "distilbert-base-uncased" is a smaller version of BERT, known for its faster performance and similar accuracy.
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

#  Set format for PyTorch
tokenized_train.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
tokenized_test.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

from transformers import DataCollatorWithPadding

# Data collator for padding the inputs to the same length
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Step 7: Load model
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# Import necessary libraries
import numpy as np
from datasets import load_metric

# Define the function to compute evaluation metrics (accuracy and F1 score)
def compute_metrics(eval_pred):
    # Load the accuracy and F1 score metrics from Hugging Face's datasets library
    load_accuracy = load_metric("accuracy")
    load_f1 = load_metric("f1")

    # Extract the logits and labels from the evaluation predictions
    logits, labels = eval_pred

    # Convert logits to predicted labels by taking the argmax (index of max value) across the last axis
    predictions = np.argmax(logits, axis=-1)

    # Compute accuracy using the loaded accuracy metric
    accuracy = load_accuracy.compute(predictions=predictions, references=labels)["accuracy"]

    # Compute F1 score using the loaded F1 metric
    f1 = load_f1.compute(predictions=predictions, references=labels)["f1"]

    # Return a dictionary with the computed metrics
    return {"accuracy": accuracy, "f1": f1}

from transformers import TrainingArguments, Trainer

# Step 5: Define the Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=2,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    logging_dir="./logs",
    logging_steps=10,
)

# Step 6: Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
)

# Step 7: Train the model
trainer.train()

# Step 10: Evaluate
trainer.evaluate()

from sklearn.metrics import classification_report
import numpy as np

preds_output = trainer.predict(tokenized_test)
preds = np.argmax(preds_output.predictions, axis=1)

# Assuming tokenized_test has 'label' field
labels = preds_output.label_ids

print(classification_report(labels, preds))

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Step 1: Get predictions from the trainer
predictions = trainer.predict(tokenized_test)
y_pred = predictions.predictions.argmax(-1)
y_true = predictions.label_ids

# Step 2: Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Step 3: Plot confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])
disp.plot(cmap='Blues', values_format='d')
plt.title("Confusion Matrix")
plt.show()

from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Assume your model and tokenizer are already defined
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

# Train the model (not shown)

# Save the model and tokenizer to the specified directory
model.save_pretrained("/content/sample_data/BERT SENTIMENT ANALYSIS")
tokenizer.save_pretrained("/content/sample_data/BERT SENTIMENT ANALYSIS")